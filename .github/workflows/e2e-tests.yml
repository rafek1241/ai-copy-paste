name: E2E Tests

on:
  pull_request:
    branches: [master, main]
    types: [opened, synchronize, ready_for_review]
  push:
    branches: [master, main]
  workflow_dispatch:
    inputs:
      skip_build_cache:
        description: 'Skip build cache'
        required: false
        default: 'false'
        type: boolean

# Cancel in-progress runs for same PR
concurrency:
  group: e2e-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  e2e-tests:
    name: E2E Tests (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45

    # Only run on non-draft PRs or on pushes
    if: github.event.pull_request.draft == false || github.event_name != 'pull_request'

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04]
        # For cross-platform E2E tests, uncomment the following:
        # os: [ubuntu-22.04, windows-latest, macos-latest]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Rust stable
        uses: dtolnay/rust-toolchain@stable

      - name: Rust cache
        uses: swatinem/rust-cache@v2
        with:
          workspaces: './src-tauri -> target'
          key: e2e-${{ matrix.os }}

      # Linux dependencies
      - name: Install Linux dependencies
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libwebkit2gtk-4.1-dev \
            libappindicator3-dev \
            librsvg2-dev \
            patchelf \
            libgtk-3-dev \
            libsoup-3.0-dev \
            javascriptcoregtk-4.1 \
            webkit2gtk-driver \
            xvfb \
            x11-utils \
            libxcb-shape0-dev \
            libxcb-xfixes0-dev

      # macOS dependencies (if needed)
      - name: Install macOS dependencies
        if: runner.os == 'macOS'
        run: |
          # Most dependencies come with Xcode
          echo "macOS ready"

      # Windows dependencies (if needed)
      - name: Install Windows dependencies
        if: runner.os == 'Windows'
        run: |
          # Most dependencies come with Visual Studio Build Tools
          echo "Windows ready"

      - name: Install npm dependencies
        run: npm ci --legacy-peer-deps

      - name: Install tauri-driver (cargo)
        run: |
          cargo install tauri-driver || echo "tauri-driver may already be installed"

      - name: Build TypeScript
        run: npm run build

      - name: Verify frontend build
        run: |
          echo "=== Checking frontend dist ==="
          ls -la dist/ || echo "dist folder not found!"
          if [ ! -f "dist/index.html" ]; then
            echo "ERROR: dist/index.html not found"
            exit 1
          fi
          echo "Frontend build verified"

      - name: Build Tauri app (Release)
        run: |
          # Use release build to properly embed frontend
          # Debug builds connect to devUrl, release builds use frontendDist
          cd src-tauri
          cargo build --release
        env:
          TAURI_SIGNING_PRIVATE_KEY: ""
          TAURI_SIGNING_PRIVATE_KEY_PASSWORD: ""

      - name: Verify Tauri build
        run: |
          echo "=== Checking Tauri build output ==="
          ls -la src-tauri/target/release/ | head -30
          echo ""
          echo "=== Looking for binaries ==="
          find src-tauri/target/release -maxdepth 1 -type f -executable 2>/dev/null | head -10
          echo ""
          # Check both possible binary names
          for name in "ai-context-collector" "ai-copy-paste-temp"; do
            if [ -f "src-tauri/target/release/$name" ]; then
              echo "Found binary: $name"
              file "src-tauri/target/release/$name"
            fi
          done
          echo ""
          # Verify the dist folder is in place
          echo "=== Verifying dist folder ==="
          ls -la dist/ | head -10

      - name: Create E2E test directories
        run: |
          mkdir -p e2e/reports
          mkdir -p e2e/logs
          mkdir -p e2e/screenshots
          mkdir -p e2e/fixtures/test-data

      - name: Create test fixtures
        run: |
          cat > e2e/fixtures/test-data/sample.ts << 'EOF'
          export const hello = "world";
          export const add = (a: number, b: number) => a + b;
          EOF

          cat > e2e/fixtures/test-data/sample.js << 'EOF'
          const x = 42;
          console.log(x);
          module.exports = { x };
          EOF

          cat > e2e/fixtures/test-data/sample.md << 'EOF'
          # Hello

          This is a test file for E2E testing.
          EOF

          cat > e2e/fixtures/test-data/sample.json << 'EOF'
          {"key": "value", "number": 123}
          EOF

      # Run E2E tests with Xvfb on Linux
      - name: Run E2E tests (Linux)
        if: runner.os == 'Linux'
        run: |
          # Check binary exists in release directory (check both possible names)
          echo "=== Checking Tauri binary ==="
          ls -la src-tauri/target/release/ | head -20 || echo "Release dir not found"

          BINARY_PATH=""
          for name in "ai-context-collector" "ai-copy-paste-temp"; do
            if [ -f "src-tauri/target/release/$name" ]; then
              BINARY_PATH="src-tauri/target/release/$name"
              echo "Binary found: $BINARY_PATH"
              file "$BINARY_PATH"
              break
            fi
          done

          if [ -z "$BINARY_PATH" ]; then
            echo "ERROR: No Tauri binary found"
            echo "Looking for any executable in release dir:"
            find src-tauri/target/release -maxdepth 1 -type f -executable 2>/dev/null
            exit 1
          fi

          # Start Xvfb
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 &
          XVFB_PID=$!
          sleep 3

          # Verify Xvfb is running
          if ! ps -p $XVFB_PID > /dev/null; then
            echo "ERROR: Xvfb failed to start"
            exit 1
          fi
          echo "Xvfb started with PID $XVFB_PID"

          # Quick test: Try to launch app directly to see if it starts
          echo "=== Quick app startup test ==="
          timeout 10 "$BINARY_PATH" &
          APP_TEST_PID=$!
          sleep 5
          if ps -p $APP_TEST_PID > /dev/null 2>&1; then
            echo "App started successfully (PID: $APP_TEST_PID)"
            kill $APP_TEST_PID 2>/dev/null || true
          else
            echo "WARNING: App may have crashed or exited quickly"
          fi
          sleep 1

          # Start tauri-driver with verbose logging
          echo "=== Starting tauri-driver ==="
          tauri-driver &
          TAURI_DRIVER_PID=$!
          sleep 3

          # Verify tauri-driver is running
          if ! ps -p $TAURI_DRIVER_PID > /dev/null; then
            echo "ERROR: tauri-driver failed to start"
            exit 1
          fi
          echo "tauri-driver started with PID $TAURI_DRIVER_PID"

          # Check if tauri-driver is listening
          sleep 2
          curl -s http://localhost:4444/status || echo "tauri-driver status endpoint not responding (this may be normal)"

          # Run E2E tests (diagnostic and app-launch only for now)
          echo "=== Running E2E tests ==="
          npx wdio run e2e/wdio.conf.ts --logLevel=info || E2E_EXIT_CODE=$?

          # Cleanup
          echo "=== Cleanup ==="
          kill $TAURI_DRIVER_PID 2>/dev/null || true
          kill $XVFB_PID 2>/dev/null || true

          exit ${E2E_EXIT_CODE:-0}
        env:
          CI: true
          WEBKIT_DISABLE_COMPOSITING_MODE: 1

      # Run E2E tests on Windows
      - name: Run E2E tests (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          # Start tauri-driver in background
          Start-Process -NoNewWindow tauri-driver
          Start-Sleep -Seconds 2

          # Run tests
          npx wdio run e2e/wdio.conf.ts
        env:
          CI: true

      # Run E2E tests on macOS
      - name: Run E2E tests (macOS)
        if: runner.os == 'macOS'
        run: |
          # Start tauri-driver in background
          tauri-driver &
          TAURI_DRIVER_PID=$!
          sleep 2

          # Run tests
          npx wdio run e2e/wdio.conf.ts || E2E_EXIT_CODE=$?

          # Cleanup
          kill $TAURI_DRIVER_PID 2>/dev/null || true

          exit ${E2E_EXIT_CODE:-0}
        env:
          CI: true

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-results-${{ matrix.os }}
          path: |
            e2e/reports/
            e2e/logs/
            e2e/screenshots/
          retention-days: 14

      - name: Upload E2E screenshots on failure
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: e2e-screenshots-${{ matrix.os }}
          path: e2e/screenshots/
          retention-days: 14

  e2e-report:
    name: E2E Test Report
    needs: e2e-tests
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Generate summary
        run: |
          echo "## E2E Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check for test results
          if [ -d "artifacts" ]; then
            for dir in artifacts/e2e-results-*; do
              if [ -d "$dir" ]; then
                os=$(basename "$dir" | sed 's/e2e-results-//')
                echo "### $os" >> $GITHUB_STEP_SUMMARY

                # Check for JUnit results
                if [ -f "$dir/reports/e2e-results-0.xml" ]; then
                  # Extract test counts from JUnit XML
                  tests=$(grep -oP 'tests="\K[0-9]+' "$dir/reports/e2e-results-0.xml" | head -1 || echo "0")
                  failures=$(grep -oP 'failures="\K[0-9]+' "$dir/reports/e2e-results-0.xml" | head -1 || echo "0")
                  echo "- Tests: $tests" >> $GITHUB_STEP_SUMMARY
                  echo "- Failures: $failures" >> $GITHUB_STEP_SUMMARY
                else
                  echo "- Results file not found" >> $GITHUB_STEP_SUMMARY
                fi
                echo "" >> $GITHUB_STEP_SUMMARY
              fi
            done
          else
            echo "No artifacts found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check for screenshots
        run: |
          if [ -d "artifacts" ]; then
            screenshot_count=$(find artifacts -name "*.png" | wc -l)
            if [ "$screenshot_count" -gt 0 ]; then
              echo "### Screenshots" >> $GITHUB_STEP_SUMMARY
              echo "Found $screenshot_count screenshot(s) from failed tests" >> $GITHUB_STEP_SUMMARY
            fi
          fi
